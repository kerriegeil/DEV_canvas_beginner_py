{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1768413a-c9cc-4d37-85c9-cfe4eb0704d7",
   "metadata": {},
   "source": [
    "# Introduction to Pandas for Working with Tabular Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db644034-050c-4ae3-b249-80d0b26e714d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "## This notebook covers\n",
    "- Pandas data structures - DataFrames and Series\n",
    "- Selecting, slicing, and querying DataFrames\n",
    "- Simple calculations with summary functions\n",
    "- Sorting and grouping data\n",
    "- Copying and renaming DataFrame columns\n",
    "- Handling missing values\n",
    "- Merging DataFrames and writing to file\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec7f935-a8ca-4192-836f-ba569d498b98",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "## Reminders\n",
    "\n",
    "Remember, you can use Jupyter's built-in table of contents (hamburger on the far left) to jump from heading to heading.\n",
    "\n",
    "---\n",
    "\n",
    "This notebook should run in the Anaconda base environment. We'll discuss more about environments later, but for now look for something like the words \"Python3\" or \"base\" at the top right of this notebook. If it says \"No Kernel\", go to the Kernel tab, select Change Kernel, then select the Python3 or base kernel in the pop up window.\n",
    "\n",
    "---\n",
    "\n",
    "To run cells in this notebook place your cursor in the cell you want to run, then hit Shift+Enter.\n",
    "\n",
    "---\n",
    "\n",
    "To turn on line numbers for code cells go to View menu and click Show Line Numbers.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a10e75-ab62-4798-b547-2b80b1e896be",
   "metadata": {},
   "source": [
    "# I. Importing Necessary Packages\n",
    "The following code will load the packages we'll need for this notebook. Packages are collections of code that add functionality to the core Python. It's best practice to import everything we need in one place at the top of a notebook or script.\n",
    "\n",
    "The \"as pd\" part of the Pandas import statement below is giving the Pandas package an alias in our notebook. This way when we want to use functions from the Pandas package we can type, for example, ```pd.DataFrame()``` instead of the longer ```pandas.DataFrame()```. An alias can be anything really, but \"pd\" is the alias that the Pandas user community has settled on.\n",
    "\n",
    "This particular package was installed to your computer during the installation of Anaconda, so we can simply import it here instead of having to take the extra step of downloading it first. We'll cover how to download and import additional packages in a subsequent notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb74e754-e313-4365-b6f0-ff58f88b352c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5b7ab8-1fba-45b1-ac14-585722000206",
   "metadata": {},
   "source": [
    "Packages that extend the core Python language, such as the one we imported above usually have a website where we can find tons of helpful information. Package websites may include, for example, \"Getting Started\" tutorials, in-depth user guides, an API reference that documents the particulars of every single available function, and instructions on where to ask the user community questions, submit bug reports, or make software contributions. \n",
    "\n",
    "**If you need help, package websites are one of the first places you should look. Let's take a quick look at the Pandas website [https://pandas.pydata.org/](https://pandas.pydata.org/)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6343d55f-4c6e-4ae3-a9dd-10264e3aa3fd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# II. Introduction to Pandas Data Structures\n",
    "\n",
    "On the Pandas website, the package developers describe the project's goal: Pandas \"aims to be the fundamental high-level building block for doing practical, real world data analysis in Python. Additionally, it has the broader goal of becoming the most powerful and flexible open source data analysis / manipulation tool available in any language.\"\n",
    "\n",
    "Pandas is a powerful tool for working with tabular data such as data stored in spreadsheets, databases, or other table-like formats. The main data structure Pandas uses to hold data is called the *DataFrame*. A DataFrame is a 2-dimensional (rows and columns) structure that can store data of different types including strings, integers, floating point values, categorical data, and more. DataFrames are like a spreadsheet - think of a table of data with column headings, row numbers, and data values. Each column of a Pandas DataFrame is its own data structure called a *Series*. Both data structures (DataFrame and Series) have an *index*. We can think of an index, for now, as a row or line number, but it can be anything, like a date or any other text.\n",
    "\n",
    "Below are schematics of what a Pandas DataFrame and Series look like, where the darker grey boxes would hold headers (column names) and indexes (row names/numbers), while the lighter grey boxes would hold the data values.\n",
    "\n",
    "<table><tr>\n",
    "<!-- <td> <img src=\"https://pandas.pydata.org/docs/_images/01_table_dataframe.svg\" alt=\"schematic of a dataframe\" width=\"700\"/> </td>\n",
    "<td> <img src=\"https://pandas.pydata.org/docs/_images/01_table_series.svg\" alt=\"schematic of a series\" width=\"200\"/> </td> -->\n",
    "<td> <img src=\"images/01_table_dataframe.svg\" alt=\"schematic of a dataframe\" width=\"700\"/> </td>\n",
    "<td> <img src=\"images/01_table_series.svg\" alt=\"schematic of a series\" width=\"200\"/> </td>\n",
    "</tr></table\n",
    "         \n",
    "(Image Source: [Pandas Docs Getting Started Tutorial](https://pandas.pydata.org/docs/getting_started/intro_tutorials/01_table_oriented.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb11aedc-3ab0-41a1-be18-0c096d1381f0",
   "metadata": {},
   "source": [
    "## Making a DataFrame from Scratch\n",
    "\n",
    "We can make a DataFrame programmatically, as opposed to reading data from a file. For example, let's create one from lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345bb90e-d62a-46f5-9bd0-986115883156",
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfColumnNames = [\"rank\", \"county_fips\",\n",
    "                     \"per_capita_income\", \"median_household_income\",\n",
    "                     \"population\", \"num_households\"]\n",
    "\n",
    "listOfTuples = [(1,'089',32223,59730,98468,35297),\n",
    "                (2,'121',27183,56159,145165,52539),\n",
    "                (3,'073',27399,50075,57786,21237),\n",
    "                (4,'033',25065,59734,166234,56641),\n",
    "                (5,'059',23547,49620,140298,50185),\n",
    "                (6,'047',23111,44550,194029,69384),\n",
    "                (7,'149',22079,40404,48773,18941),\n",
    "                (8,'045',21935,44494,43929,17380),\n",
    "                (9,'081',21831,39049,82910,32086),\n",
    "                (10,'131',21691,43728,17786,6165)]\n",
    "\n",
    "# create DataFrame from lists\n",
    "countyData = pd.DataFrame(listOfTuples, columns = listOfColumnNames)\n",
    "        \n",
    "# if placed on a line by itself, we get pretty output of the DataFrame    \n",
    "countyData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b00b56-7d33-4543-a593-bdb366e8cf7b",
   "metadata": {},
   "source": [
    "Our data is now in a Pandas DataFrame, which has 6 named columns of data (6 Series) as well as a row index (the bold column without a header).\n",
    "\n",
    "Isolating a Series from a DataFrame would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa2e0b8-6e2c-47ff-9a43-9cf69c5e9f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "countyData[\"population\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9040be0-8ec7-4f67-8bac-7a8341e2d328",
   "metadata": {},
   "source": [
    "We can see that the rendering of a Series isn't pretty like that of a DataFrame. But notice how a Series isn't just the data values. The index remains attached and the column name is there as well. How do we know this is a Pandas Series? We can use Python's built-in function ```type()``` like we did in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cfc778-a216-4ad7-9fd2-5fced4a8de16",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(countyData[\"population\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ee1ea0-aab3-48fd-9c3a-17df4dfeb122",
   "metadata": {},
   "source": [
    "And for good measure..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c521877c-c592-470d-b090-4001fa8e99b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(countyData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382d59ce-331f-42ba-b83b-483a4f62e250",
   "metadata": {},
   "source": [
    "## Loading Tabular Data from a File into a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb518a8-8120-4b9c-9494-fc19f92118aa",
   "metadata": {},
   "source": [
    "Pandas makes it very easy to load an Excel file or other tabular data sources like .csv files into DataFrames. \n",
    "\n",
    "The .csv file extension is a common file format for tabular data. CSV stands for comma separated values. Inside a CSV file, you will see rows of data with commas used between the values of each data column (i.e., *comma delimited*). Generally, we use .csv files instead of Excel because Excel has a limit on length (1,048,576 rows). (Also, reading an Excel file with Pandas requires installation of an additional package. We'll cover that in a subsequent lesson).\n",
    "\n",
    "A raw CSV file with a header row might look like this, for example:\n",
    "\n",
    "<pre>\n",
    "Book Title,Publisher,Price\n",
    "War and Peace,Vintage Classics,12.99\n",
    "\"Our Bodies, Ourselves\",Touchstone,48.38\n",
    "Putin's Playbook,\"Simon & Schuster, Inc.\",14.49\n",
    "</pre>\n",
    "\n",
    "Notice that the second book title listed and the third publisher name have a comma in the data value. Those values are surrounded by quotation marks to avoid Pandas interpreting the comma as a new column. This is important to do when you are creating your own CSV data.\n",
    "\n",
    "Let's begin by loading an example data file that is .csv format into a Pandas DataFrame. \n",
    "\n",
    "The example data used in this notebook is college football bowl data. While this particular data may not be relevant to your job or research, the data exploration and cleaning techniques we'll work through do broadly apply to any tabular data you may have (in .csv, .xls(x) or even .txt formats).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119550c2-e718-4bc0-80cd-34bd8b8acb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "bowlData = pd.read_csv('data/collegefootballbowl.csv')\n",
    "bowlData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6add4af3-6946-42c3-93c5-bc8bb357afa0",
   "metadata": {},
   "source": [
    "Notice what gets printed to the screen when a DataFrame has many rows. We can see the first 5 and last 5 rows of data, followed by the shape (rows, columns) of the full DataFrame.\n",
    "\n",
    "Also notice some columns contain NaN values. NaN stands for \"not a number\" and usually represents a missing data value of type float. We'll cover more about different missing data values, how to handle them, and the missing data features Pandas offers later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa84948a-9c0b-4b32-84ee-664cb16ab3fe",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    \n",
    "**Sidebar about data management for tabular data:** It's important to, at minimum, create a data dictionary that describes what is in your data file, even if your data file contains column names. Often there is more information (metadata) that is required by future users of the data than just the column names and data values. Metadata for the collegefootballbowl.csv can be found in the [data/collegefootballbowl.txt](data/collegefootballbowl.txt) file, which tells future data users the original source of the data, when the data file was created, and contains a data dictionary that describes each column of data. This type of information is super important especially when your data values have units. Even future you may forget if your temperature data, for example, is in Celsius or Fahrenheit, or if your precipitation data had units of cm, mm, inches, or hundredths of inches! \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a06751-18dd-4370-9848-e3ea2ec1d22a",
   "metadata": {},
   "source": [
    "## Viewing the Head and Tail of a DataFrame\n",
    "\n",
    "```.head()``` lets us view the first N rows of a DataFrame \n",
    "\n",
    "```.tail()``` lets us view the last N rows\n",
    "\n",
    "Using either one of these methods on a DataFrame without any additional parameters will show us 5 rows. Enter an integer as a parameter to either function to view a different number of rows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668d457c-5289-4a18-8e2f-1ff5cd2316de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view first 10 rows\n",
    "bowlData.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8801da8-336b-4995-9b5d-4da8f3872518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view last 10 rows\n",
    "bowlData.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa7c684-7154-4cae-85c3-2972b0aab588",
   "metadata": {},
   "source": [
    "## Getting DataFrame Information\n",
    "\n",
    "### Shape Property: How Many Rows / Columns?\n",
    "\n",
    "```.shape``` returns a tuple (rows, columns) and is the most concise way to see how many total rows and columns are in a DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c412c2-7124-439a-ae17-215e4caba355",
   "metadata": {},
   "outputs": [],
   "source": [
    "bowlData.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5632f9a6-cd7e-47db-8092-8a04837e3168",
   "metadata": {},
   "source": [
    "### Dtypes Property: Understanding the Data Types of Each Column\n",
    "\n",
    "Often, we will need to know the data type of each column, ```.dtypes``` will give us that information.\n",
    "\n",
    "Anything that says \"object\" is probably storing strings. int64 and float64 are numerical data (numbers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670a0899-5b65-4b8b-a87a-6de4d4d74365",
   "metadata": {},
   "outputs": [],
   "source": [
    "bowlData.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fae4e0-8ec5-441a-a13c-409c674f3d4b",
   "metadata": {},
   "source": [
    "How were the data types for each column determined? \n",
    "\n",
    "When we read the .csv file using ```pd.read_csv()``` Pandas inferred the data type of each column based on the column's data values. If all values in a column appear to be integers, Pandas will infer that the column is data type int. Sometimes, there can be mistakes in our data files though. We could have a data column, for example \"winner_rank\", that should contain all integers or missing values but a data entry mistake in the file has added 1 or more values that are non-numeric in that column. A lot of real data is \"messy\" like this. In these cases where there are mixed data types in a single column, Pandas usually reads the whole column as strings and assigns a data type of object. This is in fact the case with our data columns \"winner_rank\" and \"loser_rank\", which we will investigate a bit more later.\n",
    "\n",
    "### Describe Method: Summary of Numerical Data (count, mean, std, min, quartiles, max)\n",
    "\n",
    "We can access simple statistical information for numerical data columns using the ```.describe()``` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5282162a-7672-4b4c-a77f-e2ce2c26c8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bowlData.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52591c2e-ac14-4e71-ade7-9832d2e4a3cc",
   "metadata": {},
   "source": [
    "Notice that ```.describe()``` returns statistical information only for numerical data columns. \n",
    "\n",
    "For which year was the earliest data record in our DataFrame collected? We can see the answer is the \"min\" of the \"year\" column, 1901.\n",
    "\n",
    "To see all columns, numeric or not, we can add a parameter to ```.describe()``` like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a17f6af-eea5-45b4-b80d-d40783592cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bowlData.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4391c2-599c-48f8-96fa-077434ffe32a",
   "metadata": {},
   "source": [
    "We can now see 3 additional statistics that operate only on non-numeric data columns: \"unique\", \"top\", and \"freq\". NaN appears wherever the data type is not appropriate for the statistic. \n",
    "\n",
    "Who is the most common sponsor and how many times were they a sponsor? \n",
    "\n",
    "Looking to the \"sponsor\" column, the \"top\" row indicates the most common data value is Outback Steakhouse and the \"freq\" row indicates that Outback Steakhouse was a sponsor 26 times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdeacfea-d1bc-4874-b722-c83f18e7c305",
   "metadata": {},
   "source": [
    "### Info Method: Understanding all Fields, Null Values, Dtypes, Shape, Size, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a7218d-3c06-4c39-9b05-836e15302f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "bowlData.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6867edf4-b290-4d50-bde2-0883d2e0b62b",
   "metadata": {},
   "source": [
    "# III. Selecting/Slicing Data with .loc[]\n",
    "\n",
    "The Pandas function ```.loc[]``` allows us to directly access rows by index and columns by name ```.loc[row_index,column_name]``` or to access all rows of data based on a conditional ```.loc[condition]```. Let's take a look, starting with selecting by row index and column name.\n",
    "\n",
    "## Single Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d850fcaa-cd2b-4f97-8118-0e2fca0a5a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT a single cell - the attendance column where row index is 1\n",
    "bowlData.loc[1, 'attendance']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2d120b-76e4-4eec-ac36-3caafa4a3ad0",
   "metadata": {},
   "source": [
    "If we put an integer in the row part of ```.loc[row_index,column_name]```, Pandas assumes this is the index of the row."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1c9de2-801b-42c2-8451-20081587830b",
   "metadata": {},
   "source": [
    "## Single row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93aff753-e1c4-4988-a31b-0890067f1b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT whole row of data where row index is 1\n",
    "bowlData.loc[1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d4f577-4552-4bfa-bcdd-fc2089ad7cf1",
   "metadata": {},
   "source": [
    "A colon by itself means \"everything\", here specifically it means all columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9854480b-bf3f-4f0b-9991-82edd5944bc4",
   "metadata": {},
   "source": [
    "## Single Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f028a711-8950-4184-b8cd-5d95a2822fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT a single column of data, just the attendance column\n",
    "bowlData.loc[:, 'attendance']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a488738-ca8c-42b3-af78-a75af45408c2",
   "metadata": {},
   "source": [
    "Here the colon by itself means all rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76af8e3a-43af-4708-aeeb-130e5bd14bd5",
   "metadata": {},
   "source": [
    "## Slice of Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988115e8-db87-4f7d-a357-13b30121e0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT a slice of rows where row index is 1 to 6\n",
    "bowlData.loc[1:6, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0479692d-9151-4c91-8860-c4deffdeda7c",
   "metadata": {},
   "source": [
    "A colon between two integers means a slice (meaning get multiple rows).\n",
    "\n",
    "Notice that a slice of a Pandas DataFrame is inclusive of the ending row index 6, such that a slice of rows 1:6 returns 6 rows. Just a quick note that this is not always the case with other Python packages. Array slicing with the Numpy or Xarray packages, for example, are exclusive of the ending index, but we'll cover that in a subsequent lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37db1f2-0b75-4da1-ac1a-9c5a8b892e44",
   "metadata": {},
   "source": [
    "## All Rows, Slice of Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef74292-941c-4783-ab03-e6dc25ab039f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT all rows but only a slice of columns from year to winner_rank \n",
    "bowlData.loc[:, 'year':'winner_rank']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ef2796-b265-419a-9cbd-76e28a04c3fc",
   "metadata": {},
   "source": [
    "Notice that when the output exceeds twenty lines, Jupyter will format nicely and show a bit at the beginning and a bit at the end. The same thing will happen if the output has too many columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421ab211-d2f2-41da-b495-f909fceda29c",
   "metadata": {},
   "source": [
    "## Slice of Rows, Slice of Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53fd3f7-515e-466b-b2f5-a73a58c52005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT a slice of rows (index 10 to 15) and a slice of columns ('year' to 'winner_rank')\n",
    "bowlData.loc[10:15, 'year':'winner_rank']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731cc36b-afd7-4f57-a130-cf71385ae92e",
   "metadata": {},
   "source": [
    "## Slice of Rows, Particular Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b49399-d2d7-47cd-a823-d58dc79601f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT a slice of rows and two specific columns\n",
    "bowlData.loc[10:15, ['year', 'winner_rank']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c215366-4f44-4621-ad2e-ac2b6a9b5ecd",
   "metadata": {},
   "source": [
    "## All Rows Where Column has Certain Value\n",
    "\n",
    "Now, instead of putting row indexes and column names in ```.loc[]``` we'll use a condition instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7f3c5f-d570-4cee-b66d-ce030a2102a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all rows where the year column is equal to 1901\n",
    "bowlData.loc[bowlData['year'] == 1901]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d359c6-f811-45cc-afd2-da6cf0125770",
   "metadata": {},
   "source": [
    "This returned only one row. If there were more rows where the \"year\" column was equal to 1901, then there would be more rows returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bec8c89-fa1e-4368-8ea1-76c9d0974256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all rows where the sponsor column is missing data\n",
    "bowlData.loc[bowlData['sponsor'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22142e50-ff2f-4bf6-9a52-dae121b51ed2",
   "metadata": {},
   "source": [
    "## Rows Based on Column Comparison\n",
    "\n",
    "If we want to return all the games where the winner was ranked below (higher number) the loser (lower number), we could do it this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd43d9c4-d270-4bbf-bc27-3342accee392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all rows where the winner ranked below the loser\n",
    "bowlData.loc[bowlData['winner_rank'] > bowlData['loser_rank']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f2c26e-96be-488e-b02f-c16a9b08e154",
   "metadata": {},
   "source": [
    "**Be careful in your analysis! Do you notice anything suspicious about the results that were returned?**\n",
    "\n",
    "We're looking for cases where the underdog won. For example, a team that was ranked 20th, beat the team that was ranked 10th (see index 1429). If we look closely in the results above we can see multiple examples of results returned that are NOT what we asked for. For example, index 1427 shows the winner had a better rank (8th) than the loser (13th). So, we've got incorrect results but did not receive an error message. \n",
    "\n",
    "**Why did this happen??** \n",
    "\n",
    "This comes back to the data type of the columns \"winner_rank\" and \"loser_rank\". A ranking should be a numerical data type (such as int or float). If these columns had a numeric data type then we wouldn't have experienced any problems using the greater than operator. But if those columns contain non-numeric data (dtype object, which usually indicates string data) then that could yield unexpected results when comparing if one string is greater than another. Let's take a look at the data type for \"winner_rank\" and \"loser_rank\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c36f71c-b1cc-4ba7-8296-cae8cc523d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "bowlData[['winner_rank', 'loser_rank']].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48918ba0-2f67-4db8-b440-70bb55fbf329",
   "metadata": {},
   "source": [
    "Uh oh! We can see that Pandas determined the data type of those columns to be object, which is non-numeric. We discussed briefly already why this might happen. This should indicate to us that there must be some messy data somewhere in those columns, some data values that don't look like numbers. And due to this, Pandas is treating those data columns like strings instead of numeric numbers. So, when we ask if the \"winner_rank\" is greater than the \"loser_rank\", the process of comparing two string values with the greater than operator returns unexpected results.\n",
    "\n",
    "When beginning work with any dataset it is best to do some data cleaning first to make sure your columns are of the expected data types and remove any whacky data values in order to avoid problems like this one. Otherwise, you might easily miss the fact that your code isn't doing what you intended.\n",
    "\n",
    "Let's see what the problem is, clean up those two columns, ensure their dtype is numeric, and try our selection again.\n",
    "\n",
    "We'll start by using a Pandas function that we haven't seen yet: ```.unique()``` on the \"winner_rank\" column of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f967bc2-6828-4d16-bc17-c05b6803b552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at all the unique data values in the winner_rank column\n",
    "bowlData['winner_rank'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1bc19c-8d32-40aa-9049-167d2fff5ec6",
   "metadata": {},
   "source": [
    "Look at that! Someone has entered a value of \"Pennsylvania\" in the column of \"winner_rank\" somewhere in the data file. That doesn't make sense at all. Notice, we also have some missing data (NaN), but that shouldn't cause us problems here.\n",
    "\n",
    "Let's force the \"winner_rank\" data column to be numeric using another Pandas function that we haven't seen yet: ```pd.to_numeric()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32a5bad-b688-4124-a999-868c976f69c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reassign all the values in the column winner_rank with numeric data values\n",
    "# any value that does not look like a number will be changed to the missing data value\n",
    "bowlData['winner_rank'] = pd.to_numeric(bowlData['winner_rank'], errors = 'coerce')\n",
    "\n",
    "# look again at the unique data values\n",
    "bowlData['winner_rank'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cdc77d-34e2-4e02-9fbb-2dac67564078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the new data type\n",
    "bowlData['winner_rank'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f284048-5283-431b-9277-67a4eaea911a",
   "metadata": {},
   "source": [
    "How did we know what parameters to enter in ```pd.to_numeric()``` function? That information can be found in the Pandas documentation on the [API reference page for ```pd.to_numeric()```](https://pandas.pydata.org/docs/reference/api/pandas.to_numeric.html). Or we could have done a quick Google search for something like \"how to use pandas to_numeric\", which will bring up a great AI generated answer (at least it does in the USA at the time this notebook was created), the link to the Pandas API reference page, and many other websites that demonstrate how to use the function.\n",
    "\n",
    "Let's check out what the problem is with \"loser_rank\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e6bacc-c77f-4ac1-9dbb-df1c66717a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at all the unique data values\n",
    "bowlData['loser_rank'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2da2223-ee0d-4014-b2e7-5c81a18802d0",
   "metadata": {},
   "source": [
    "Again, there's a nonsensical data value \"TN\". We'll do the same process to clean the \"loser_rank\" column and convert to a numeric data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cf8aa1-d73b-4292-8da9-4774b8bd7433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reassign all the values in the column with numeric data values\n",
    "# any value that does not look like a number will be changed to the missing data value\n",
    "bowlData['loser_rank'] = pd.to_numeric(bowlData['loser_rank'], errors = 'coerce')\n",
    "\n",
    "# look again at the unique data values\n",
    "bowlData['loser_rank'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb2449c-29e1-4788-bff7-97fad1c08fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the new data type\n",
    "bowlData['loser_rank'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aeb32b0-8e6b-45fa-a853-8b25ec0c75ad",
   "metadata": {},
   "source": [
    "Lastly, let's try the row selection by column comparison again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17a1321-f0e6-414f-b8e7-9959833d360c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finds all rows where the winner ranked below the loser\n",
    "bowlData.loc[bowlData['winner_rank'] > bowlData['loser_rank']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37de099b-33fd-4859-8aea-c72680ea922d",
   "metadata": {},
   "source": [
    "Notice how the result now returns 215 rows whereas before we got 255 rows. Our original selection returned 40 incorrect results!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15aa77ce-1fe2-4aa4-b3d8-f4b921da1305",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\"> \n",
    "\n",
    "## Exercise 1: Selecting with .loc[]\n",
    "\n",
    "Use ```.loc[]``` to select row indexes 100 through 110 and the three columns \"year\", \"winner_points\", \"loser_points\" from the ```bowlData``` DataFrame.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa95ace-d96f-4ef5-88f7-010a6849131b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef689798-e760-4a2f-929f-7fb506eed2b9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> \n",
    "\n",
    "Now select the rows where attendance is greater than 100,000.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cb3266-eb6f-4bef-be2a-4e1f7c02803f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bac31a-e070-42e2-a85a-c7453862fdc7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> \n",
    "    \n",
    "How many games in the DataFrame had attendance greater than 100,000? Don't count the rows in your answer above, determine your answer programmatically.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3eb815d-6ea2-4cc9-b41e-4b59e7468c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd3295a-d095-4207-a6e1-a4c5b7c8d1a0",
   "metadata": {},
   "source": [
    "# IV. Selecting data with .query()\n",
    "\n",
    "Similar to ```.loc[]```, the ```.query()``` method can be used to select rows in a DataFrame based on a condition. Inside the ```.query()``` we can put a condition that looks a little bit like a database query. You may like this method if you have experience working with databases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655c4c6b-98d6-4834-b7b6-20c81d0bd19a",
   "metadata": {},
   "source": [
    "## Rows Where Column has Certain Numerical Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24af4575-2e34-40b9-a98d-6ed394c4a49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all rows where year is equal to 1901\n",
    "bowlData.query(\"year == 1901\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bd8256-fb6c-48bd-96b9-9459b79fb3de",
   "metadata": {},
   "source": [
    "## Rows Where Column has String Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6fc563-60b9-4476-80bb-6d03c94b3163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all rows where the bowl_name is \"Rose Bowl\"\n",
    "# notice the single quotes around the string \"Rose Bowl\"\n",
    "bowlData.query(\"bowl_name == 'Rose Bowl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c979285-cbe7-4d64-b918-42545f2abc4e",
   "metadata": {},
   "source": [
    "## Rows Based on Substring Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcf2a60-8037-4c44-ade3-84f2a3ca3aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab all rows where the word \"State\" appears in the winner_tie column\n",
    "bowlData.query(\"winner_tie.str.contains('State')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efcf6df-0570-4f1e-a21a-ce544a6bcd0e",
   "metadata": {},
   "source": [
    "## Rows Based on Column Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8d2ddb-2292-4e57-a01a-414739a274b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all rows where the loser ranked higher (smaller number) than the winner (larger number)\n",
    "bowlData.query(\"loser_rank < winner_rank\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340d906f-6310-4bd8-8a88-d5c58cd609ab",
   "metadata": {},
   "source": [
    "## Rows Based on Comparison with a Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c757dbd8-1f41-4052-97e4-a9eda277d295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's calculate the mean winner_points x 2\n",
    "# .mean() is calculating the mean of the entire winner_points column\n",
    "twiceTheMean = bowlData.winner_points.mean() * 2\n",
    "\n",
    "# then, we can use that value to query for winners with more than twice the mean\n",
    "bowlData.query(\"winner_points > @twiceTheMean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8602a21-a87c-45b7-adc3-eec65294854a",
   "metadata": {},
   "source": [
    "## Multiple Criteria Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba255f5-9e21-4f4e-9d34-68820a538031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all the rows where the winner is Alabama, \n",
    "# AND Alabama had more than 50 points, \n",
    "# AND they weren't ranked number 1.\n",
    "bowlData.query(\"(winner_tie == 'Alabama') and (winner_points > 50) and (winner_rank > 1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7ba623-726d-430d-9ea1-d08f82c0f6bd",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> \n",
    "    \n",
    "## Exercise 2: Selecting with .query()\n",
    "\n",
    "Use ```.query()``` to find rows where the \"winner_tie\" column contains \"State\", the \"bowl_name\" contains \"Rose\", and the attendance is greater than 75,000.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82eae8a-9d2e-479a-ba70-795a6bce6c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf44e2bf-33b0-483b-a8b4-d474d8adb2c6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> \n",
    "    \n",
    "Show programmatically how many rows your query found.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2f48ed-aae1-4968-9b6b-3a8079bcd75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d708ce95-f025-4673-8fb2-d7a4f201ec3e",
   "metadata": {},
   "source": [
    "# V. Querying a DataFrame without .query()\n",
    "\n",
    "Different syntax can be used to accomplish the same data queries we covered above without using ```.query()```. This syntax may be preferable if you don't have experience working with databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f0f6c6-bbf3-497f-94dc-966632b80065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all rows where year is equal to 1901 without using .query\n",
    "\n",
    "# copy of code from the .query() section for reference\n",
    "# bowlData.query(\"year == 1901\") \n",
    "\n",
    "# alternative query syntax\n",
    "bowlData[bowlData.year == 1901]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d57ef67-8e0d-4f14-8d92-0e711633438b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all rows where the bowl_name is \"Rose Bowl\"\n",
    "\n",
    "# copy of code from the .query section for reference\n",
    "# bowlData.query(\"bowl_name == 'Rose Bowl'\") \n",
    "\n",
    "# alternative query syntax\n",
    "bowlData[bowlData.bowl_name == \"Rose Bowl\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb937af4-5de8-4e11-89e3-e03b11b2de24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab all rows where the word \"State\" appears in the winner_tie column\n",
    "\n",
    "# copy of code from the .query section for reference\n",
    "# bowlData.query(\"winner_tie.str.contains('State')\")\n",
    "\n",
    "# alternative query syntax\n",
    "bowlData[bowlData.winner_tie.str.contains('State')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47c03f8-ccf9-469d-9964-6fbda5f22191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all rows where the loser ranked higher (smaller number) than the winner (larger number) using .query\n",
    "\n",
    "# copy of code from the .query section for reference\n",
    "# bowlData.query(\"loser_rank < winner_rank\")\n",
    "\n",
    "# alternative query syntax\n",
    "bowlData[bowlData.loser_rank < bowlData.winner_rank]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330cd6e8-b785-48aa-b1db-7ac8919f1d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows Based on Comparison with a Variable\n",
    "\n",
    "# copy of code from the .query section for reference\n",
    "# twiceTheMean = bowlData.winner_points.mean() * 2\n",
    "# bowlData.query(\"winner_points > @twiceTheMean\")\n",
    "\n",
    "# alternative query syntax\n",
    "twiceTheMean = bowlData.winner_points.mean() * 2  # the same\n",
    "bowlData[bowlData.winner_points > twiceTheMean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868f4a2b-e40a-4479-a0e3-eae6f38a6df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all the rows where the winner is Alabama, \n",
    "# AND Alabama had more than 50 points, \n",
    "# AND they weren't ranked number 1.\n",
    "\n",
    "# copy of code from the .query section for reference\n",
    "# bowlData.query(\"(winner_tie == 'Alabama') and (winner_points > 50) and (winner_rank > 1)\")\n",
    "\n",
    "# alternative query syntax\n",
    "bowlData[(bowlData.winner_tie == 'Alabama') & (bowlData.winner_points > 50) & (bowlData.winner_rank > 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb97e202-2970-4079-bb85-4da7688a281e",
   "metadata": {},
   "source": [
    "Notice the difference here where we're using ```&``` to link multiple conditions together instead of how in the previous notebook Python Language Basics we learned how to use the boolean operator ```and``` to link multiple conditions. The ```&``` is called  \"bitwise and\" whereas Python ```and``` is called \"logical and\". Unless using ```.query()```, Pandas requires bitwise and, bitwise or, and bitwise not, which are written as: ```&```, ```|```, ```~```. Otherwise, we will get an error. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf3d82e-9901-420b-9a32-1514d831773c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> \n",
    "    \n",
    "## Exercise 3: Query without using .query()\n",
    "\n",
    "Without using ```.query()``` repeat the query from exercise 2 (find rows where the \"winner_tie\" column contains \"State\", the \"bowl_name\" contains \"Rose\", and the \"attendance\" column is greater than 75,000).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585f7795-ba8e-4138-a8e6-2bccce9b979a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2334e5d-aabd-4983-af0f-27c1eab5bfcc",
   "metadata": {},
   "source": [
    "# VI. DataFrame Manipulation\n",
    "\n",
    "## Summary Functions\n",
    "\n",
    "Pandas offers a handful of summary functions that can be applied to a column or columns of a DataFrame (Series objects). These functions are:\n",
    "\n",
    "- ```.sum()``` Sum values of each object. \n",
    "- ```.count()``` Count non-NA values of each object. \n",
    "- ```.median()``` Median value of each object. \n",
    "- ```.quantile([0.25,0.75])``` Quantiles of each object. \n",
    "- ```.min()``` Minimum value in each object. \n",
    "- ```.max()``` Maximum value in each object. \n",
    "- ```.mean()``` Mean value of each object. \n",
    "- ```.var()``` Variance of each object. \n",
    "- ```.std()``` Standard deviation of each object.\n",
    "\n",
    "We won't cover all of these, but let's try a few."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac01c4b4-f448-4ad7-89b2-232ff5cbeb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the mean of winner_points\n",
    "print(\"Winners had an average of \", bowlData['winner_points'].mean())\n",
    "\n",
    "# the median of loser_points\n",
    "print(\"Losers had a median of \", bowlData['winner_points'].median())\n",
    "\n",
    "# lowest score of a winning team\n",
    "print(f\"The highest score of a losing team is {bowlData['loser_points'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c599e41-166f-40e2-b493-64324491fff8",
   "metadata": {},
   "source": [
    "Wow, 61 points and still a loss... ooof."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fe08c1-9cc1-436e-9a97-68e59d5f7fad",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> \n",
    "    \n",
    "### Exercise 4: Find the standard deviation of a column\n",
    "\n",
    "Create a print statement similar those above to print the standard deviation of the \"winner_points\" column and \"loser_points\" column. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da598a68-a731-49e9-8bef-cb493f29df07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6332d081-de63-4ea5-a9b9-1468253a0ef3",
   "metadata": {},
   "source": [
    "## Sorting Data\n",
    "\n",
    "Let's begin with a simple sort on a numeric data column with Pandas ```.sort_values()```. This function will sort numeric data in descending order by default unless we provide the parameter ```ascending=True```. \n",
    "\n",
    "First, we'll double check that the \"year\" column of the DataFrame is numeric (we'll get into why we're doing this in a minute). Then we'll sort by \"year\" ascending and return only the \"year\" and \"winner_points\" columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6e8575-6eb9-4318-becd-9897b120fa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if year column is numeric\n",
    "bowlData.year.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056a47dd-ad91-4feb-971a-ef0d0f6781f0",
   "metadata": {},
   "source": [
    "Excellent, we should be good to proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f399cb-2983-4cb4-950a-920b48e82bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort specific columns by year\n",
    "bowlData[['year', 'winner_points']].sort_values(by = ['year'], ascending = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26dfb52-76cd-4dca-a8a8-09aaf634f2fd",
   "metadata": {},
   "source": [
    "Now, let's do a more complex sort. Sort by \"winner_points\" descending to find the highest score by an upset winner (winner ranked lower than loser).\n",
    "\n",
    "We'll use the columns \"winner_rank\" and \"loser_rank\" to accomplish this sort (which we have already converted to numeric data) as well as the \"winner_points\" column.\n",
    "\n",
    "Let's double check the data type of the \"winner_points\" column before we sort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfb5a56-d00d-49c2-81e6-a290f8bc8500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if winner_points column is numeric\n",
    "bowlData.winner_points.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99372a0a-6118-48c6-9ca0-de6fbfc565d0",
   "metadata": {},
   "source": [
    "Great, another numeric data column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da5fb79-0eae-4210-8d9d-472b818ce8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by winner_points to find the highest score by an upset winner\n",
    "bowlData[bowlData.winner_rank > bowlData.loser_rank].sort_values(by = ['winner_points'], ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3b6885-130d-4c59-b9e5-ea55d29a8ba0",
   "metadata": {},
   "source": [
    "Notice that our code returned a lot of rows but we'll find the answer to our question in the first row in the \"winner_points\" column: 70.\n",
    "\n",
    "How would we do the same sort but return only the highest score by an upset winner as opposed to all the data rows that were returned in the code above?\n",
    "\n",
    "Since we are sorting with ```ascending = False```, the highest \"winner_points\" is located in the first row of returned data. In this case, the index of the first row is 727. We don't know ahead of time what the index value of the first row of results with be though. Don't worry! We can grab the first row of the sorted results using the integer row position 0 with ```.iloc[]``` as opposed to using the row index with ```.loc[]```.\n",
    "\n",
    "From the results above we expect the output of the following code to be 70."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9541969-88da-4b40-a337-e5b18cf89eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by the winner_points to find the highest score by an upset winner\n",
    "bowlData[bowlData.winner_rank > bowlData.loser_rank].sort_values(by = ['winner_points'], ascending = False).iloc[0].loc['winner_points']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69713c85-1897-469f-86b4-00f62f48f0aa",
   "metadata": {},
   "source": [
    "Wow! What just happened? We used ```.iloc[0]``` to select only the first row of the results returned by sorting and ```.loc['winner_points']``` to return the value of the \"winner_points\" column from the results return by ```.iloc[0]```. \n",
    "\n",
    "Are you beginning to see the power of Pandas? We can string together many functions in a row to achieve what we're looking for.\n",
    "\n",
    "Does sorting work on strings? \n",
    "\n",
    "Yes, but we have to be careful! If our strings only contain letters, ```.sort_values()``` will sort from A to Z or Z to A as we'd expect. \n",
    "\n",
    "But, if our strings contain numbers or letters and numbers together, ```.sort_values()``` may not return the sort order that we're expecting. This is because Pandas sorts strings character by character using *lexicographic order*. This is also why we've been double checking that our columns with numbers are numeric and not object data type. \n",
    "\n",
    "Before we move on, let's work through a quick example of what happens if when Pandas sorts strings that contain numbers in lexicographic order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32639c1-e94e-4810-855c-a597b4ec1c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DataFrame with a column of data that looks like numbers, but are actually strings\n",
    "df = pd.DataFrame({'ranking': ['1', '3', '5', '2', '4', '10']})\n",
    "print(df.ranking.dtype)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d860c3-d39e-49a6-8c30-9711316f6770",
   "metadata": {},
   "source": [
    "The \"ranking\" column that looks like numeric data is actually strings. What happens when we sort ascending?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba28a8f8-01f7-4a23-8f79-7e7a8d6f7600",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by = ['ranking'], ascending = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8afad1-22f2-4e18-a394-b0af299d5420",
   "metadata": {},
   "source": [
    "This is just something to be aware of. If you want to keep your numerical-looking data as strings but sort in numerical order, there would be some extra steps to execute. We won't cover that here but if you're interested you can find plenty of information on how to accomplish that on the web. For example, [this user question and answer on stackoverflow.com](https://stackoverflow.com/questions/37693600/how-to-sort-dataframe-based-on-particular-stringcolumns-using-python-pandas). That being said, it's probably a good idea to ensure any numerical-looking data columns are assigned numerical data types in order to avoid any unexpected issues caused by string numbers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf9b602-10e7-4ee4-bad4-b294c1350378",
   "metadata": {},
   "source": [
    "## Grouping Data\n",
    "\n",
    "```.groupby()``` will partition data into groups which we can then operate on using functions like```.mean()```, ```.sum()```, etc. Let's start with a super simple example before we try to use ```.groupby()``` on our ```bowlData```. Pretend we have been observing two falcons and two parrots and are keeping data records on their maximum observed flight speed in miles per hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f0d05b-a2be-487f-8eb6-4a767bb98cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a new very simple DataFrame \n",
    "birds = pd.DataFrame({'species' : ['falcon', 'falcon', 'parrot', 'parrot'],\n",
    "                   'individual' : ['f01', 'f02', 'squawky', 'pretty boy'],\n",
    "                   'age_class' : ['adult', 'adult', 'juvenile', 'adult'],\n",
    "                   'max_speed_mph' : [230., 240., 35., 40.]})\n",
    "birds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc6c5fa-23a0-489a-a8da-17a2948d7497",
   "metadata": {},
   "source": [
    "We can use ```.groupby()``` to find the average max speed of each species. We'll first group the DataFrame rows by the \"species\" column. Then, for each group (there will be two groups: falcon rows and parrot rows) take the mean of the \"max_speed_mph\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fb0a36-7b30-4ff4-86a2-27eef38e55fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "birds.groupby(\"species\")[\"max_speed_mph\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59a289e-79f8-4cb9-8212-a78a75561cc5",
   "metadata": {},
   "source": [
    "Nice! The average max speed of all the falcons is 235 mph and the average max speed of all the parrots is 37.5 mph.\n",
    "\n",
    "Pandas ```.groupby()``` will work similarly on our much larger ```bowlData``` DataFrame.\n",
    "\n",
    "Let's group by \"winner_tie\" (which is the name of the winning team) and then find the average number of points the winning team scored (\"winner_points\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81ba502-1ea0-4652-aac4-64335a113e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "avgPointsByWinners = bowlData.groupby(\"winner_tie\")[\"winner_points\"].mean()\n",
    "avgPointsByWinners"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cd1fb3-afe7-420a-9ead-a811239f5ec8",
   "metadata": {},
   "source": [
    "With the ```bowlData``` DataFrame, it is a little more obvious to see the default behaviour of the ```.groupby()``` with regard to how the function sorts the result. ```.groupby()``` will sort ascending on the grouping column (here, \"winner_tie\") when returning the result, which is why our result is sorted by \"winner_tie\" from A to Z. This is the default behavior unless we specify ```sort=False``` as a parameter in ```.groupby()```. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f95db8b-5e79-4d96-9eb1-21710e8f4e0a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> \n",
    "\n",
    "### Exercise 5: Using .groupby() \n",
    "\n",
    "Find the maximum observed flight speed for each species in the birds DataFrame.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473f3ef8-eb08-461b-a486-2f7612cc03bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5c58c8-1c43-49fa-8911-ca3e90e51411",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<div class=\"alert alert-info\"> \n",
    "This next one is challenging and uses a function we haven't covered yet. See if you can work it out!\n",
    "\n",
    "For each species, find the name of the fastest individual. \n",
    "\n",
    "Hint: you will need to use ```.loc()```, ```.groupby()```, and also a function called ```.idxmax()``` (in that order). See if you can figure it out with some help from the web. The doc page for [.idxmax()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.idxmax.html) and [this user question and answer(s) from stackoverflow.com](https://stackoverflow.com/questions/39964558/pandas-max-value-index) may get you part of the way to the answer.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883f74bb-0593-4ae3-b53b-18605ce23770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb63aa1-27b7-4988-8508-7f29edaadd1a",
   "metadata": {},
   "source": [
    "## Renaming Columns\n",
    "\n",
    "To change column names, the easiest way is this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9bad0f-b3c3-45c6-a32e-cdebed5e3420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary where key is the old name and value is the new name\n",
    "columnMap = {\"mvp\" : \"most_valuable_player\", \"winner_tie\" : \"winner\"}\n",
    "\n",
    "# then use the .rename() function\n",
    "bowlData = bowlData.rename(columns = columnMap, errors = \"raise\")\n",
    "\n",
    "bowlData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201c2032-484e-4204-b6ec-5a65aae5ed2c",
   "metadata": {},
   "source": [
    "## Handling Missing Values\n",
    "\n",
    "Sometimes we need to assess how much of our data is missing to make sure any statistics we apply to the data are robust. \n",
    "\n",
    "Sometimes having the missing data value (NaN) in our data is beneficial. Many functions will simply ignore missing data or propagate missing data values. For example, if we were to add one data value to another and the values were NaN + 5, the result would be NaN. This is often desirable.\n",
    "\n",
    "But in some cases, like for various machine learning techniques, we may need to ensure that there are no missing data values present in our data at all. In this case we may need to fill the missing data values with a different number or drop rows with missing data from our DataFrame entirely.\n",
    "\n",
    "Let's look at the Pandas functions that can help us with missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb03885-cdb3-4dba-b598-e0ecb2b33b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show how many missing values are present in each column\n",
    "bowlData.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0947c588-ac9b-4b2c-9240-6e29f2e49426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at all the rows where winner_rank is missing\n",
    "bowlData[bowlData['winner_rank'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384ebac7-3f77-4420-96e5-e0d0101008e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace all missing values with 0 \n",
    "# notice we are making a copy of our data by saving the DataFrame to a new variable here\n",
    "betterBowlData = bowlData.fillna(0)\n",
    "\n",
    "# let's see if any missing values are still present - shouldn't be\n",
    "betterBowlData.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42932d0d-38c9-4583-99f3-363bba074627",
   "metadata": {},
   "source": [
    "Let's pause for a minute to think about what we just did. We filled all NaN with 0. Does that make sense? Does it make sense to have \"winner_points\" = 0 or \"sponsor\" = 0? We can see how filling NaN with another value may end up being confusing later. So think carefully if you really need to replace missing data values, or if filling with another value like 0 will work for your analysis. \n",
    "\n",
    "Instead of filling NaN with a different value, we may need to drop full data rows if there are any missing values present. This is how we can do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebd2988-4ef2-459e-8ffc-dbb9ab0cac8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all rows where at least 1 column of data is NaN\n",
    "lessBowlData=bowlData.dropna(how = 'any')\n",
    "lessBowlData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745f455a-c01b-47ac-8cbc-d28812bbdffb",
   "metadata": {},
   "source": [
    "When we first loaded ```bowlData```, we originally had 1527 rows of data and now after dropping all rows that have at least one missing value, we are left with only 269 rows of data.\n",
    "\n",
    "If the goal was to drop all rows where all columns contain the missing data value, we could use ```.dropna(how='all')```.\n",
    "\n",
    "## Creating New Columns Derived from Existing Columns\n",
    "\n",
    "Pandas allows us to easily use existing columns to calculate new data and save the calculations into a new column in the DataFrame. \n",
    "\n",
    "Here's an example. Let's define a blowout as when the winning team beats the losing team by 21 or more points. The task now is to create a new column in our DataFrame that indicates whether each game (row) in our dataset was a blowout. We'll fill the column with values of True or False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3d6205-8424-42b2-8d6b-fea0f5103de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bowlData['blowout'] = bowlData.winner_points - bowlData.loser_points >= 21\n",
    "bowlData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5876724-ad9f-42f2-a493-3e33439e4350",
   "metadata": {},
   "source": [
    "It's that simple! There is no looping required. Pandas is smart enough to do the subtraction row by row and fill the result in the appropriate place all on its own. Writing a loop would be much slower, which is often the case with Python. It's most efficient to use the built-in capabilities of whatever packages you're working with. Try to avoid looping wherever you can."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68383e0-bb1a-435e-aa3c-87846aa62163",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> \n",
    "    \n",
    "### Exercise 6: Copy an existing column to a new column\n",
    "\n",
    "Modify the DataFrame ```bowlData``` by copying the \"year\" column to a new column called \"year_copy\". Print ```bowlData``` to the screen to check your work.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820b31dd-f79b-45a8-be92-f6fccb0e2e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e323abf0-65e7-44ba-bfa7-d14c4d4f036d",
   "metadata": {},
   "source": [
    "## Merging DataFrames Together\n",
    "\n",
    "Let's pretend we have additonal college football bowl data hanging out in a separate CSV file. The additional data has the same 'id' information (row index value) as in data/collegefootballbowl.csv, but not all id's (1-1527) are present. How do we join this additional data to the ```bowlData``` DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb08e669-2a41-4f47-9055-f98ac72639c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "moreBowlData = pd.read_csv('data/morecollegefootballbowldata.csv')\n",
    "moreBowlData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c730cdcf-3a40-4d8f-8d4c-fb22ba795bba",
   "metadata": {},
   "source": [
    "Oh wow, that's not much data! But let's merge it into ```bowlData``` anyway. The merge column will be the \"id\" column since that is the only common column between the two data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1032d63e-1d08-4621-9335-6dfcb91287a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "allBowlData = pd.merge(bowlData, moreBowlData, how = 'outer', on = 'id')\n",
    "allBowlData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a2c00c-a4ee-40c9-9a98-82f9481e70b9",
   "metadata": {},
   "source": [
    "We can see that for the three id's where we have \"tickets_sold\" and \"best_selling_concession\" data, those data values now appear in the merged DataFrame. And everywhere else in those two columns got filled with NaN. There are tons of ways to merge DataFrames. Check out the [```pd.merge()``` API reference](https://pandas.pydata.org/docs/reference/api/pandas.merge.html) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcd53be-6504-4c0b-89a0-30a5c451720d",
   "metadata": {},
   "source": [
    "# VII. Writing a DataFrame to a File\n",
    "\n",
    "Pandas has a function ```.to_csv()``` for writing a DataFrame to a .csv file.\n",
    "\n",
    "Let's write our bowl data to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d87da19-5c4e-4250-8368-bfa26d56c368",
   "metadata": {},
   "outputs": [],
   "source": [
    "bowlData.to_csv(r'data/bowlData.csv', index = None, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc42da04-d7bc-454d-ba9b-818c51fb02af",
   "metadata": {},
   "source": [
    "Locate the file you just wrote, open it, and see what it looks like. If you open the file with a double click in JupyterLab it will look like a spreadsheet- this is Jupyter's CSV viewer. If you right click the file in JupyterLab, choose Open With, then choose Editor to open the file, you should see the comma separated header and data values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1304e8e2-ce80-4128-ac09-de2418d48c18",
   "metadata": {},
   "source": [
    "# VIII. Pandas Datetimes\n",
    "\n",
    "*Datetimes* are a special type of object that represent a date and a time. Python has a built-in datetime data type that we did not previously cover because Pandas' datatimes offer much more functionality. In this section we'll cover the Pandas datetime objects *Timestamp* and *DatetimeIndex*.\n",
    "\n",
    "## Creating Datetimes with Pandas\n",
    "\n",
    "The string format of a datetime looks like ```YYYY-MM-DD hh:mm:ss.ns```. The date part of the object in years, months, and days comes before the space. The time part comes after the space in hours, minutes, seconds, and seconds fraction. The highest precision of a Pandas datetime object is nanoseconds but not all datetimes need to be that precise. \n",
    "\n",
    "Pandas stores single datetimes as Timestamp objects and sequences of datetimes as DatetimeIndex objects. We can create a Timestamp object either with ```pd.Timestamp()``` or ```pd.to_datetime()```. We can create a DatetimeIndex object either with ```pd.to_datetime()``` or ```pd.date_range()```.\n",
    "\n",
    "Let's create our first datetimes. The Pandas functions for creating Timestamps can accept date inputs in a range of different formats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9abc237-f298-43ec-bf0e-ffdacedb5727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert an indivual date string into a Pandas Timestamp object\n",
    "# all of these will result in the same Timestamp\n",
    "\n",
    "print('pd.to_datetime()')\n",
    "print(pd.to_datetime('2025-01-01')) \n",
    "print(pd.to_datetime('2025/01/01')) \n",
    "print(pd.to_datetime('1/1/2025')) \n",
    "print(pd.to_datetime('2025.01.01')) \n",
    "print(pd.to_datetime('Jan 1, 2025')) \n",
    "print(pd.to_datetime('20250101')) \n",
    "\n",
    "print('pd.Timestamp()')\n",
    "print(pd.Timestamp('2025-01-01')) \n",
    "print(pd.Timestamp('2025/01/01')) \n",
    "print(pd.Timestamp('1/1/2025')) \n",
    "print(pd.Timestamp('2025.01.01')) \n",
    "print(pd.Timestamp('Jan 1, 2025')) \n",
    "print(pd.Timestamp('20250101')) \n",
    "print(pd.Timestamp(2025,1,1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5492201-3347-4ef8-bfea-7e43116f8bdb",
   "metadata": {},
   "source": [
    "There are some differences between these two functions in what types of date inputs can be accepted though. See the Pandas API Reference for [```pd.Timestamp()```](https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.html) and [```pd.to_datetime()```](https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html) for details.\n",
    "\n",
    "Notice how Timestamp objects will always display like ```YYYY-MM-DD hh:mm:ss``` even if we don't provide hours, minutes, and seconds, they will default to zero. Also, if we leave off the month or day, Pandas will default to the first month and first day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f611848b-6b46-4515-b68f-413fd1c51556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas will fill out the rest of the Timestamp if we don't provide it in the input string\n",
    "print(pd.Timestamp('2025-01')) \n",
    "print(pd.Timestamp('2025')) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23c949e-6915-4968-9be0-82c58a464625",
   "metadata": {},
   "source": [
    "Inputting a sequence of dates into ```pd.to_datetime()``` will return a DatetimeIndex object containing data of type datetime64. The [ns] next to the data type below indicates the precision of the datetime is nanoseconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fa447f-ecba-4c4c-9c4d-2b6d364c6ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = ['2025-01-15','2025-03-12','2025-10-02','2025-02-28','2025-12-20']\n",
    "pd.to_datetime(dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bb1e8b-580e-42cb-b740-0e754ad62674",
   "metadata": {},
   "source": [
    "We can create a sequence of datetimes from a starting point to an ending point with the ```pd.date_range()``` function. The frequency parameter lets us easily create sequences of datetimes spaced at intervals. \n",
    "\n",
    "Below we create DatetimeIndex objects with daily, monthly, and 6-hourly frequency. All the available frequency options are listed in the [Pandas User Guide](https://pandas.pydata.org/docs/user_guide/timeseries.html#timeseries-offset-aliases). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f496010-ce5b-4a83-a982-c3f9d0fac7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create daily datetimes (D = daily)\n",
    "print(pd.date_range('2025-01-01','2025-01-31',freq='D')) \n",
    "\n",
    "# create monthly datetimes at the start of each month (MS = month start)\n",
    "print(pd.date_range('2025-01-01','2025-12-31',freq='MS'))\n",
    "\n",
    "# create hourly datetimes every 6 hours (6h = 6-hourly)\n",
    "print(pd.date_range('2025-01-01 00', '2025-01-01 18',freq='6h'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df15453a-d355-4c82-9bd9-8ebf33b08a99",
   "metadata": {},
   "source": [
    "As we see above with 6 hourly frequency, an integer can be added into the frequency string to create datetimes spaced at multiples of hours (or minutes, days, months, etc).\n",
    "\n",
    "## Datetime Properties\n",
    "\n",
    "Datetimes have components and properties that we can access by calling various methods directly on the datetime object. The full list of datetime properties is located in the [Pandas User Guide section on timeseries](https://pandas.pydata.org/docs/user_guide/timeseries.html#time-date-components).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c214c2-d542-4dd6-a72f-a4aae6d61b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a Timestamp object for demonstration\n",
    "one_timestamp = pd.to_datetime('2025-05-01')\n",
    "print(one_timestamp)\n",
    "\n",
    "# a DatetimeIndex object for demonstration\n",
    "datetime_index = pd.date_range('2025-01-01','2025-12-31',freq='MS')\n",
    "print(datetime_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f955319-6cd1-4e74-a5a7-c34d50a324f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accessing properties of a Timestamp object\n",
    "print(one_timestamp)\n",
    "print('-------------------')\n",
    "print('year', one_timestamp.year)\n",
    "print('month', one_timestamp.month)\n",
    "print('day', one_timestamp.day)\n",
    "print('hour', one_timestamp.hour)\n",
    "print('day of year', one_timestamp.dayofyear)\n",
    "print('day of week', one_timestamp.dayofweek)\n",
    "print('quarter', one_timestamp.quarter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcd2b34-6b28-434c-9d6a-180141d9b4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accessing properties of a DatetimeIndex object\n",
    "print(datetime_index)\n",
    "print('--------------------------------------------------------------------------------')\n",
    "print('year', datetime_index.year)\n",
    "print('month', datetime_index.month)\n",
    "print('day', datetime_index.day)\n",
    "print('hour', datetime_index.hour)\n",
    "print('day of year', datetime_index.dayofyear)\n",
    "print('day of week', datetime_index.dayofweek)\n",
    "print('quarter', datetime_index.quarter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17af9b34-0476-4060-8b40-de5450618b10",
   "metadata": {},
   "source": [
    "Timestamp and DatetimeIndex objects can also exist inside of Pandas Series and DataFrame structures. A DatetimeIndex could be a column of our DataFrame (Series) or we could use it to time-index our DataFrame (which we'll cover in the next subsection). If our datetimes are a Series in a DataFrame, we can access the object properties through the ```.dt``` accessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c1029b-d519-49ee-a333-ca0d190108ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make DatetimeIndex a Series in a DataFrame for demonstration\n",
    "df = pd.DataFrame(datetime_index,columns=['DATE'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f119825-95ff-499f-b913-afc5458de63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# access components of datetimes when they are stored in a Series\n",
    "# a Series is returned\n",
    "df.DATE.dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55216fb-4988-4650-a035-c6875986e297",
   "metadata": {},
   "source": [
    "## Timedeltas for Math with Dates\n",
    "\n",
    "*Timedelta* objects (data type timedelta64) are differences in datetimes, expressed in difference units, e.g. days, hours, minutes, seconds. They can be both positive and negative.\n",
    "\n",
    "We can use timedeltas to add or subtract a fixed amount of time from a datetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4288c607-79a7-41c4-8e9a-0dfc00bd4cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create timedelta object\n",
    "delta = pd.Timedelta('6h')\n",
    "delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabdad5d-8211-428a-a696-598fc9da24c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add timedelta to datetimes\n",
    "datetime_index + delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d1606f-402e-4fab-a8ac-acd2d1555fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtract timedelta from datetimes\n",
    "datetime_index - delta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9644fa-e139-4b23-9f6a-7325541ca237",
   "metadata": {},
   "source": [
    "Notice how easy it was to subtract 6 hours from these datetimes. If we kept our dates as strings instead of datetimes, this task would require us to write a significant amount of code. Datetimes and timedeltas allow us to use simple addition and subtraction instead of having to code up something much more complicated.\n",
    "\n",
    "## Working with Time-Indexed Data\n",
    "\n",
    "One of the most powerful applications of Pandas datetimes is for time-indexed data. This means using the DatetimeIndex object as the index in a DataFrame. This would be appropriate for data that occur over time where time has significant meaning to the data values, like daily observations of tornado occurrences, for example, or any other timeseries of data.\n",
    "\n",
    "For this section we'll work with daily severe weather counts (tornados, severe wind, and severe hail) for the state of Mississippi. This data was obtained for the years 2004-2023 from the [NOAA National Weather Service Storm Prediction Center website](https://www.spc.noaa.gov/climo/summary/) and compiled into a single data file ```data/NOAA_SevereWeather/NOAA-SPC_SevereWxCounts_MS_2004-2023.csv``` for our use here.\n",
    "\n",
    "Some of the things we can do with time-indexed data like the severe weather counts are:\n",
    "- resampling in time, e.g., daily counts --> annual counts\n",
    "- grouping in time, e.g., find the long-term average number of tornados that occur in each month\n",
    "- math with dates, e.g., find the number of days between tornado occurrences\n",
    "\n",
    "First, we'll load the data into a DataFrame. We can tell Pandas to create datetimes through the use of parameters in the ```pd.read_csv()``` function. To use the \"Date\" column of data as the index of the DataFrame we can use the parameter ```index_col='Date'```. To turn the dates into datetimes (a DatetimeIndex) we can use the parameter ```parse_dates=['Date']```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae436e40-a42b-4dfb-92e1-8ae4f9480f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data into DataFrame, converting dates to datetimes and using them as the df index\n",
    "wx_df = pd.read_csv('data/NOAA_SevereWeather/NOAA-SPC_SevereWxCounts_MS_2004-2023.csv', \n",
    "                 usecols=['Date','Tornado','Wind','Hail'], \n",
    "                 parse_dates = ['Date'],\n",
    "                 index_col='Date')\n",
    "wx_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a71a879-04f8-48be-a474-991a3d7d68ad",
   "metadata": {},
   "source": [
    "There are 20 years of data: 20 * 365 days + 5 leap days = 7305 rows of data.\n",
    "\n",
    "All the data columns are counts of severe weather occurrences, so if there are no messy data values then Pandas should have assigned those columns an integer data type. Let's double check the type of the data columns and index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200b8a6a-5c46-46bd-896f-71f3ce3120e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at data types\n",
    "print(wx_df.dtypes)\n",
    "type(wx_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d75f0e-f03a-4ca1-9a02-dcedf6d95a6f",
   "metadata": {},
   "source": [
    "We now have a time-indexed DataFrame of daily counts of severe weather occurrences in the state of Mississippi from 2004-2023. \n",
    "\n",
    "### Resampling in Time\n",
    "\n",
    "Because the index of our DataFrame is datetimes, we can easily resample the data in time, e.g., daily counts --> annual counts.\n",
    "\n",
    "Pandas DataFrame ```.resample()``` works similarly to ```.groupby()```. ```.groupby()``` works on a column of data given a certain condition, whereas ```.resample()``` works on a DatetimeIndex given a certain time alias. Here, we use the alias ```'A'``` for annual. The list of resampling alias options can be found in the [Pandas User Guide](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#period-aliases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3b4bec-dff2-42ca-86a0-9083733c578a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample daily to annual counts\n",
    "df_annual = wx_df.resample('A').sum()\n",
    "df_annual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e2231b-a438-4d33-9aad-e9bbb669f592",
   "metadata": {},
   "source": [
    "Notice that just like with ```.groupby()``` we need to also apply some sort of mathematical function like ```.sum()```."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e128289-5d1d-4080-bf4b-ce83e74d704e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> \n",
    "\n",
    "### Exercise 7: Resample data using datetimes\n",
    "\n",
    "Resample ```wx_df``` to obtain a DataFrame where each row contains the sum of one month of each type severe weather observation (your result should retain the columns \"Tornado\", \"Wind\", and \"Hail\").\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5322e49a-8c09-4485-b3fe-0e61d484ea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code here\n",
    "\n",
    "wx_df.resample('M').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf67a76-fd09-45ff-b626-587ae99f2fa5",
   "metadata": {},
   "source": [
    "### Grouping in Time\n",
    "\n",
    "We can also use datetimes to easily group in time to calculate things like the long-term average number of occurences of each type of severe weather per month of the year.\n",
    "\n",
    "First, we'll programmatically get the number of data years in the dataset by accessing the ```.year``` property of the DatetimeIndex and then use the Pandas function ```.nunique()``` to get the set of unique years in the data (total number of years)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3437b5b7-1d7b-4910-84ce-fbc91a47c37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# programmatically determine number of years in the dataset\n",
    "nyears = df.index.year.nunique()\n",
    "nyears"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b1d40e-6197-4a6e-8109-64baef7a787d",
   "metadata": {},
   "source": [
    "Now, we can group the entire dataset by month, sum the values in group, and divide by the total number of years in the dataset to get the average number of occurences of each type of severe weather per month of the year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d12de9-3825-4581-8945-c8713fc3942c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate long term monthly means\n",
    "df_monthly_mean = df.groupby(df.index.month).sum() / nyears\n",
    "df_monthly_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8596ded-9376-4695-b6b7-13031ddd3d65",
   "metadata": {},
   "source": [
    "### Differencing Dates\n",
    "\n",
    "Another very useful aspect of datetimes is how we can difference them. When we difference two datetimes, the result is a timedelta. Let's look at an example. We'll difference consecutive dates of tornado occurrences to get the number of days between tornados. First, let's subset our daily severe weather data to only the tornado data and drop all records when there were no tornados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b245ab-d6f0-474d-b7a4-f0892f503b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new DataFrame with only the index and Tornado column\n",
    "# double square bracket means return a DataFrame instead of a Series\n",
    "tornados = df[['Tornado']]\n",
    "\n",
    "# drop all rows with zero tornados\n",
    "tornados = tornados.query('Tornado != 0')\n",
    "\n",
    "tornados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08f727e-9e98-4202-9dd0-10b3f99fe0aa",
   "metadata": {},
   "source": [
    "Now we can use the ```.diff()``` function on the DataFrame DatetimeIndex. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ded20c-f5a8-4a70-84d3-c828bc054355",
   "metadata": {},
   "outputs": [],
   "source": [
    "tornados['timedelta_since_tornados'] = tornados.index.diff()\n",
    "print(tornados.dtypes)\n",
    "tornados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf82b75-a105-4670-a034-e811bfecbe44",
   "metadata": {},
   "source": [
    "Pandas executes ```.diff()``` on our DataFrame index as ```diff[i] = index[i] - index[i-1]``` (subtracting the previous index value). That's why the first result is the missing value NaT which stands for \"Not a Time\", the datetime equivalent of NaN.\n",
    "\n",
    "Notice the data type of the \"timedelta_since_tornados\" column is data type timedelta64. To convert timedelta objects that are in a Series to a numerical data type we can use the ```.dt``` accessor. ```.dt.days``` will pull out the day component of the timedelta objects into a numerical data type. The list of attributes you can access from timedeltas can be found in the [Pandas API Reference for ```pd.Timedelta()```](https://pandas.pydata.org/docs/reference/api/pandas.Timedelta.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcb598a-1002-4c49-867c-abf4bc9299f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tornados['days_since_tornados'] = tornados['timedelta_since_tornados'].dt.days\n",
    "print(tornados.dtypes)\n",
    "tornados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dc2619-716d-47e8-b9a7-c6158797d07a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> \n",
    "    \n",
    "# IX. Exercise: Putting it All Together\n",
    "\n",
    "Use Pandas to read, clean, manipulate, and aggregate weather observations.\n",
    "\n",
    "## Read the data file\n",
    "Read the file at ```data/weatherdata.csv``` into a Pandas DataFrame and render the DataFrame to the screen. Don't specify any parameters besides the filename when reading the file into a DataFrame.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0cf4c4-8402-49e6-8a4c-b33795d116b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ded0cb9-e9ba-4484-ac96-43309f5ceb81",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> \n",
    "\n",
    "## Clean the data\n",
    "\n",
    "Look at all the columns of data. Do you see any mistakes?\n",
    "\n",
    "Replace 'LosAngeles' at index 3 with 'Los Angeles'.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32612b2d-e39c-4317-b6bb-b47fcd404615",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3467a56c-5a71-4302-b028-a63feff66b20",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> \n",
    "Show the data type of each column. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b6cf83-d0c1-4ca6-87a5-eb2bb61263ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5c246d-fe5f-4f41-a867-8fa14914f065",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> \n",
    "What data type is the \"windspeed_knots\" column? \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c950ddcb-3e7c-4784-b1c8-47aa174b2333",
   "metadata": {},
   "source": [
    "Type your answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abf2ecb-2a56-46ac-b6d3-7317567638bc",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Why did Pandas assign that data type to \"windspeed_knots\"?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110f087b-1658-4338-9432-b9671e5f3e9d",
   "metadata": {},
   "source": [
    "Type your answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b532849-75da-4dfb-98bb-a4d1914d944c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Judging by the data values in the \"windspeed_knots\" column, what data type should \"windspeed_knots\" probably be?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dfa38e-feda-45c3-8a3e-534b74eee0eb",
   "metadata": {},
   "source": [
    "Type your answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccbdc2f-4b03-4140-b8bb-47458a1348fb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Force the \"windspeed_knots\" column to be numeric, then show that the data type of the column did in fact change.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8e0abe-8c28-435d-ba07-471ad287d5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe2af43-be8e-4d3c-835c-23235524ffdc",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> \n",
    "    \n",
    "## Create a new column of data\n",
    "\n",
    "Create a column of boolean data called \"IsRainy\" that indicates whether there was precipitation.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d119e9b-2426-4adc-9bb8-f7a650caee02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2dcbf4-fdc9-476e-9d5d-da1ef28825b9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> \n",
    "\n",
    "## Calculate average temperature by city\n",
    "\n",
    "Calculate the average temperature for each city and save the result as new variable called ```avgT```.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19110cad-e27d-4e69-bf94-705aef3fd0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9765d01-9bcc-42f3-9c10-aafe3efc3e50",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> \n",
    "    \n",
    "Looking at the ```df``` DataFrame and the ```avgT``` results, how many data values were used to calculate the average New York temperature? \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d222a471-f968-495d-a543-445fd1c108bb",
   "metadata": {},
   "source": [
    "Type your answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08c72a1-9b32-4862-8379-2444062c207e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> \n",
    "\n",
    "## Convert date strings to datetimes\n",
    "\n",
    "Convert the string values in the \"dates\" column to a DatetimeIndex. You don't need to reset the DataFrame's index to the \"dates\" column though, just convert the string data in the \"dates\" column to datetimes.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978b4b1e-cb06-462d-9f21-a02c11f8a1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c8de70-ca94-4f4e-a02c-d695bc24a655",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> \n",
    "    \n",
    "## Sort the DataFrame by date, ascending\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2170d7-c6a5-4a09-a46e-044fc0f4c41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15f4ae0-4d74-4445-8644-79e44eb13077",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "\n",
    "**Sidebar about sorting:** If you have dates in your data, it's best to convert them from string values to datetime objects. Remember how we saw earlier what can happen when sorting strings that contain numbers? If we had full months of date strings, sorting would be problematic due to the lexicographic sort order. This problem is avoided completely if you work with dates as datetimes instead of strings. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e48227f-4314-4f86-a922-4b285eb50c61",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> \n",
    "    \n",
    "## Write the DataFrame to file\n",
    "\n",
    "Write the ```df``` data to a file called ```weatherdata_yourname.csv```, replacing yourname with your first name. Do not include the index column, but do include the column names.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8e697e-4ae4-46bf-90c2-2ea2abb5fa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bc69db-015b-4e89-9d58-b099676a4ecf",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> \n",
    "\n",
    "Does your csv file look like this inside?\n",
    "\n",
    "<img src=\"images/weatherdata.png\" alt=\"contents of cvs file\" width=\"400\"/>\n",
    "\n",
    "If so, congratulations! You've successfully completed this exercise.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502a970b-12ed-4446-b07c-b180d1a0d14e",
   "metadata": {},
   "source": [
    "# X. At a Glance: Language Covered\n",
    "\n",
    "The Pandas functionality that we covered at a glance...\n",
    "\n",
    "## Pandas functions\n",
    "\n",
    "```pd.DataFrame()```, ```pd.read_csv()```, ```pd.to_numeric()```, ```pd.merge()```, ```pd.to_datetime()```, ```pd.Timestamp()```, ```pd.date_range()```, ```pd.Timedelta()```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebaee89-4a3e-4888-aa88-cf90b2fa2221",
   "metadata": {},
   "source": [
    "## Pandas data structure (DataFrame or Series) methods \n",
    "\n",
    "```.head()```, ```.tail()```, ```.describe()```, ```.info()```, ```.unique()```, ```.query()```, ```.mean()```, ```.median()```, ```.max()```, ```.std()```, ```.sum()```, ```.sort_values()```, ```.groupby()```, ```.idxmax()```, ```.rename()```, ```.isna()```, ```.fillna()```, ```.dropna()```, ```.to_csv()```, ```.resample()```, ```.diff()```, ```.nunique()```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489e7111-ad31-4b70-bf31-4f2e5b6f24c4",
   "metadata": {},
   "source": [
    "## Pandas data structure (DataFrame or Series) attributes and accessors\n",
    "\n",
    "```.shape```, ```.dtypes```, ```.loc```, ```.iloc```, ```.dt```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27809429-0ce6-4744-9a9a-3892ca3f18c6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "# XI. Learning More About Pandas\n",
    "\n",
    "For more about Pandas, start on the Pandas website where you can find:\n",
    "\n",
    "- a nice cheat sheet https://pandas.pydata.org/docs/getting_started/index.html\n",
    "- a long list of community developed tutorials https://pandas.pydata.org/docs/getting_started/tutorials.html#communitytutorials\n",
    "- the user guide, which contains a bunch of 10 minute learning guides as well as more in-depth guides by topic https://pandas.pydata.org/docs/user_guide/index.html\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042ba720-de69-4a98-bc96-76a2a35277ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base]",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
